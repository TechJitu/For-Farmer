{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f326a481-d487-4b8c-9f8e-b96a585160dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import sys\n",
    "import joblib # <-- Import joblib to save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7b0191-36a5-4c34-9160-2b27772f0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the crop yield data from a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Successfully loaded data from '{filepath}'.\")\n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        print(\"Please make sure 'crop_yield.csv' is in the same directory as this script.\")\n",
    "        sys.exit(1) # Exit the script if file not found\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the data: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the data and separates features (X) and target (y).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Preprocessing Data ---\")\n",
    "    \n",
    "    # 1. Handle missing values (if any)\n",
    "    # Let's check for NaNs and drop them for this model\n",
    "    nan_count = df.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Found and dropping {nan_count} missing values.\")\n",
    "        df = df.dropna()\n",
    "    else:\n",
    "        print(\"No missing values found. Data is clean.\")\n",
    "        \n",
    "    # 2. Define Features (X) and Target (y)\n",
    "    \n",
    "    # --- CRITICAL STEP: Prevent Data Leakage ---\n",
    "    # 'Yield' is often calculated as 'Production' / 'Area'. \n",
    "    # Using 'Production' to predict 'Yield' would be \"cheating\" as it\n",
    "    # already contains the answer. We MUST drop it.\n",
    "    if 'Production' in df.columns:\n",
    "        print(\"Dropping 'Production' column to prevent data leakage.\")\n",
    "        X = df.drop(['Yield', 'Production'], axis=1)\n",
    "    else:\n",
    "        X = df.drop('Yield', axis=1)\n",
    "        \n",
    "    y = df['Yield']\n",
    "    \n",
    "    # 3. Identify feature types\n",
    "    categorical_features = ['Crop', 'Season', 'State']\n",
    "    numeric_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "    \n",
    "    # Verify all columns are accounted for\n",
    "    all_features = categorical_features + numeric_features\n",
    "    missing_cols = set(X.columns) - set(all_features)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns are in the data but not used in the model: {missing_cols}\")\n",
    "        \n",
    "    print(f\"Target (y): 'Yield'\")\n",
    "    print(f\"Categorical Features (X): {categorical_features}\")\n",
    "    print(f\"Numerical Features (X): {numeric_features}\")\n",
    "\n",
    "    return X, y, categorical_features, numeric_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b17ce8c-be7f-4dda-bbfd-11c96fb8878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_pipeline(categorical_features, numeric_features):\n",
    "    \"\"\"\n",
    "    Creates a scikit-learn pipeline to process data and train the model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building ML Pipeline ---\")\n",
    "\n",
    "    # Create a transformer for numerical features\n",
    "    # For Random Forest, scaling isn't necessary, so we'll just use 'passthrough'\n",
    "    # If we were using an SVM or Neural Net, we'd use StandardScaler() here.\n",
    "    numeric_transformer = 'passthrough'\n",
    "\n",
    "    # Create a transformer for categorical features\n",
    "    # This will convert text (like 'Assam', 'Rice') into numbers (0s and 1s)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Create a \"preprocessor\" that applies the correct transformer to the correct columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough' # Pass through any columns we didn't explicitly mention\n",
    "    )\n",
    "    \n",
    "    # --- Define the Model ---\n",
    "    # We use a RandomForestRegressor. It's powerful, fast, and good at \n",
    "    # explaining its predictions.\n",
    "    # n_jobs=-1 uses all available CPU cores for training.\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15)\n",
    "    \n",
    "    # --- Create the Full Pipeline ---\n",
    "    # This pipeline will first run the preprocessor, then train the model.\n",
    "    ml_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    print(\"Pipeline built successfully.\")\n",
    "    return ml_pipeline\n",
    "\n",
    "def train_and_evaluate(ml_pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Splits data, trains the model, and evaluates its performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training and Evaluating Model ---\")\n",
    "    \n",
    "    # 1. Split data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training data size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing data size:  {X_test.shape[0]} samples\")\n",
    "\n",
    "    # 2. Train the model\n",
    "    print(\"Training model... (This may take a moment)\")\n",
    "    ml_pipeline.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # 3. Evaluate the model\n",
    "    print(\"\\n--- Model Evaluation (on Test Data) ---\")\n",
    "    y_pred = ml_pipeline.predict(X_test)\n",
    "    \n",
    "    # R-squared: \"Coefficient of Determination\". 1.0 is a perfect score.\n",
    "    # It explains how much of the variance in Yield is \"explained\" by our features.\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Mean Absolute Error (MAE): The average error in our prediction.\n",
    "    # E.g., an MAE of 0.5 means our predictions are, on average, \n",
    "    # off by 0.5 (in whatever unit 'Yield' is in).\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"R-squared (R²):     {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    \n",
    "    if r2 < 0.5:\n",
    "        print(\"Warning: R-squared is low. The model may not be capturing all patterns.\")\n",
    "    elif r2 > 0.8:\n",
    "        print(\"Excellent! R-squared is high. The model explains the data well.\")\n",
    "        \n",
    "    return ml_pipeline # Return the trained pipeline\n",
    "\n",
    "def save_model(pipeline, filepath):\n",
    "    \"\"\"\n",
    "    Saves the trained ML pipeline to a file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Saving Model ---\")\n",
    "    try:\n",
    "        joblib.dump(pipeline, filepath)\n",
    "        print(f\"Model successfully saved to '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "def show_feature_importance(ml_pipeline, categorical_features, numeric_features):\n",
    "    \"\"\"\n",
    "    Extracts and displays the most important features from the trained model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Top 20 Most Important Features ---\")\n",
    "    \n",
    "    # Get the components from the pipeline\n",
    "    preprocessor = ml_pipeline.named_steps['preprocessor']\n",
    "    regressor = ml_pipeline.named_steps['regressor']\n",
    "    \n",
    "    # Get the raw feature importances\n",
    "    importances_raw = regressor.feature_importances_\n",
    "    \n",
    "    # Get the feature names *after* one-hot encoding\n",
    "    try:\n",
    "        onehot_features = list(preprocessor.named_transformers_['cat']\n",
    "                               .named_steps['onehot']\n",
    "                               .get_feature_names_out(categorical_features))\n",
    "        \n",
    "        # Combine numeric and one-hot encoded feature names\n",
    "        all_feature_names = numeric_features + onehot_features\n",
    "        \n",
    "        # Create a pandas Series for easy sorting\n",
    "        importances = pd.Series(importances_raw, index=all_feature_names)\n",
    "        \n",
    "        # Sort and display top 20\n",
    "        top_20 = importances.sort_values(ascending=False).head(20)\n",
    "        \n",
    "        print(top_20)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve feature names. Error: {e}\")\n",
    "        print(\"Note: Feature importance display requires scikit-learn 0.24 or newer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a26d8c-e7d3-4255-8344-ebf7b60a2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from 'crop_yield.csv'.\n",
      "Data shape: (19689, 10)\n",
      "\n",
      "--- Preprocessing Data ---\n",
      "No missing values found. Data is clean.\n",
      "Dropping 'Production' column to prevent data leakage.\n",
      "Target (y): 'Yield'\n",
      "Categorical Features (X): ['Crop', 'Season', 'State']\n",
      "Numerical Features (X): ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
      "\n",
      "--- Building ML Pipeline ---\n",
      "Pipeline built successfully.\n",
      "\n",
      "--- Training and Evaluating Model ---\n",
      "Training data size: 15751 samples\n",
      "Testing data size:  3938 samples\n",
      "Training model... (This may take a moment)\n",
      "Model training complete.\n",
      "\n",
      "--- Model Evaluation (on Test Data) ---\n",
      "R-squared (R²):     0.9795\n",
      "Mean Absolute Error (MAE): 10.1345\n",
      "Excellent! R-squared is high. The model explains the data well.\n",
      "\n",
      "--- Top 20 Most Important Features ---\n",
      "Crop_Coconut            0.846147\n",
      "Annual_Rainfall         0.031777\n",
      "Pesticide               0.021613\n",
      "Area                    0.021390\n",
      "State_Karnataka         0.019742\n",
      "Fertilizer              0.013082\n",
      "State_West Bengal       0.012557\n",
      "State_Assam             0.010838\n",
      "Crop_Year               0.006934\n",
      "State_Chhattisgarh      0.004022\n",
      "State_Telangana         0.003809\n",
      "State_Puducherry        0.002712\n",
      "State_Andhra Pradesh    0.002377\n",
      "State_Tamil Nadu        0.001970\n",
      "State_Goa               0.000489\n",
      "Season_Whole Year       0.000157\n",
      "State_Mizoram           0.000096\n",
      "Crop_Sugarcane          0.000094\n",
      "State_Kerala            0.000063\n",
      "Season_Kharif           0.000063\n",
      "dtype: float64\n",
      "\n",
      "--- Saving Model ---\n",
      "Model successfully saved to 'crop_yield_model.joblib'\n",
      "\n",
      "--- ML Workflow Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define the path to your data\n",
    "    data_filepath = 'crop_yield.csv'\n",
    "    model_save_path = 'crop_yield_model.joblib'\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = load_data(data_filepath)\n",
    "    \n",
    "    # 2. Preprocess Data\n",
    "    X, y, cat_features, num_features = preprocess_data(df)\n",
    "    \n",
    "    # 3. Build Pipeline\n",
    "    pipeline = build_model_pipeline(cat_features, num_features)\n",
    "    \n",
    "    # 4. Train and Evaluate\n",
    "    trained_pipeline = train_and_evaluate(pipeline, X, y)\n",
    "    \n",
    "    # 5. Show what the model learned\n",
    "    show_feature_importance(trained_pipeline, cat_features, num_features)\n",
    "    \n",
    "    # 6. Save the trained model for the API\n",
    "    save_model(trained_pipeline, model_save_path)\n",
    "    \n",
    "    print(\"\\n--- ML Workflow Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644c6fb-057a-441d-b025-8c6891e9c52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
